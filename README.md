# sprintmission18
# 미션개요
- 한국환경공단 에어코리아 대기오염정보 데이터를 사용, 그중에서도 특히 <시도별 실시간 측정정보 조회> API를 불러옴
- 데이터 분석 부서에서 실시간 대기오염 정보를 분석해야 하는 니즈가 있다고 가정하고
- 필요할 때 언제든지 분석할 수 있도록 실시간 측정정보를 BigQuery에 적재하는 작업
- 이 모든 과정을 airflow로 자동

## 1. 프로젝트 설계 가이드라인 (Roadmap)
데이터 엔지니어링 미션 5단계로 나누고, 각 단계를 거칠 때마다 GitHub에 기록을 남겨 포트폴리오에 활용

### Phase 1: 요구사항 분석 및 데이터 파악
- 목표: 에어코리아 API의 구조를 파악하고, 분석팀이 어떤 형태의 데이터를 원하는지 정의합니다.
- 체크리스트: API 호출 한도(Quota), 데이터 업데이트 주기(매시 정각), 제공되는 데이터 형식(JSON/XML) 확인.

### Phase 2: 데이터 파이프라인 아키텍처 설계
- 목표: 데이터가 소스(API)에서 목적지(BigQuery)까지 가는 경로를 그립니다.
- 핵심 도구: Python (추출), Airflow (스케줄링), BigQuery (저장).

### Phase 3: 환경 구축 (Infrastructure)
- 목표: 작업을 수행할 운동장을 만듭니다.
- 작업: GCP 프로젝트 생성, BigQuery 데이터셋/테이블 생성, Airflow 환경(Docker 또는 Managed 서비스) 설정.

### Phase 4: ETL 프로세스 개발 (핵심)
- Extract: API에서 데이터를 가져오는 로직.
- Transform: 분석하기 좋은 형태로 가공 (결측치 처리, 데이터 타입 변경).
- Load: BigQuery에 적재.

### Phase 5: 모니터링 및 자동화
- 목표: 장애 발생 시 알림을 받고, 매일 정해진 시간에 사고 없이 돌아가게 만듭니다.

## 2. 문제 직면 시의 인사이트 (Troubleshooting Mindset)

"API 데이터가 중간에 비어있다면?" * 인사이트: 분석팀에 '데이터 없음'을 어떻게 알릴 것인가? (Null로 둘 것인가, 이전 시간 데이터를 채울 것인가?)

"Airflow 작업이 실패했는데, 다시 돌리면 중복 데이터가 쌓인다면?"

인사이트: **Idempotency(멱등성)**의 개념을 고민해보세요. 동일한 작업을 여러 번 수행해도 결과가 같아야 합니다.

"BigQuery 비용이 너무 많이 나온다면?"

인사이트: 데이터 파티셔닝(Partitioning)과 클러스터링(Clustering)을 통해 쿼리 효율을 높이는 방법을 찾아보세요.
